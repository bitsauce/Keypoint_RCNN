{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADGxJREFUeJzt3X2sZHddx/HPF0tqRRKKIm3aGFJ8AnxqSsUC2gUhEEBsEI3EZzCpkRIVjJFoIghabSRosoAoWDSaSGKwNhYCQVqgdQubtgmCRq1PSYG2PNRatS7S/vxjzsB4vXvv3d17d74z83olN71zZvbc32l+2Xve8ztntsYYAQAA6Owhyx4AAADAboQLAADQnnABAADaEy4AAEB7wgUAAGhPuAAAAO1tTLhU1WOq6r1btt1+Evt5V1VdOH3/nKr6bFXV9PiqqvrhPezjNVX1r4vjqaoLq+qmqvpAVb2vqi6Ytl8wbbuhqq6vqvN32O9jq+qWqvqPqnrqwvbfqqqbp69fWNj+yqo6WlUfrqqXn+j/C4C9qqpzqup1J/D6G3b6+w6AzbMx4bKPbkzylOn7pyS5NckTFh5/cA/7eGOSp23Z9skkzx5jfGeS30zy6mn7TyV56xjjUJI/SPKyHfb7ySTPTPKnW7a/YYzx7UmenOR7psB5eJIXJ5lv/8mqetgexs4GqqovWfYYWG1jjDvHGK/Yut3cAmCvhMsWVfWmqvqRqnpIVb27qp605SU3JpmvZnxLkjcleWpVnZnknDHGv+z2M8YYn0zy4JZtd44x7psefi7J56fvP5bkEdP3j0xyd1WdWVU3VtU3VNWjpxWTR4wx/muM8dltft4/TP99MMkD09f9ST6R5Kzp6/4k/7Pb2Ompqp5QVUemVbl3VdXjp3lxXVX9YVW9anrd7Qt/5i1VdWj6/t3TO9wfrqpLpm2vqqq3VdW1Sb6/qi6tqvdPr/ud+UojHE9V/frCvLx8vsq8zdx62rTifENVvX6b/Vw5zb0jVfW8034gALRwxrIHcJpdVFU37PKan03yvsxWT/5yjPGhLc9/KMnvV9VDk4wkH0jyuiQfTfLhJJlO/K7cZt+/MsZ4304/fFr1+NUkPz5tem+Sd1fVS5KcmeTbxhjHqurFSd6W5N4kPzPG+LddjivTZWz/OI+rqnpnkr/LLGBfO8b43G77oK1nJbl6jPG7VfWQJH+W5KfHGEeq6vf28OdfMMb4z6p6XJI3JHn6tP3YGOP5U6TcmuTQGOPe6eTyuUn+4gCOhTVQVc9J8tVJnjzGGFX12CTft/CSxbn1t0kuHWPctXUFpqqeneTsMcalVfVlSY5U1XVjjHG6jgWAHjYtXG4ZYzxj/mC7e1zGGP9dVVcnuSrJucd5/u4kL0hy2xjjU1V1TmarMDdOrzmS5NCJDm6KobcnuXKM8TfT5t9I8ktjjHdU1YuS/FqSl44x/r6q/jnJI8cYf7WHfT8jyY8m+e7p8dcl+d4kF2QWLu+vqmvGGB8/0XHTwtVJfrGq/jjJR5J8baaQziy2t7tXYH5v1llJfruqvj6z1bjzFl4zn1tfmeQxSf58Wmj58syiF47nG5NcvxAYD2x5fj63HpXkM2OMu5JkjLH1dd+U5NKFN53OTPIVST697yNmY1XVFUlemOT2McZPLHs8bB5zcG9cKrZFVZ2b5CVJXptZJGznxiQ/n+Sm6fEnMnsn8YPTPi6ZLnnY+vX04+wv07vkf5TkmjHGNYtP5Yu/oO/O7HKxVNUzkzw0yaer6vm7HNOTkrwmyQvHGPcv7Pe+McaxaduxzE5GWU3Hxhg/N8b4wczuc7oryROn5y5eeN29VXXu9K72t07bnp3kgTHGd2R2T9XiJWDzk8hPJ/mnJM8bYxwaYzwxyVsP6FhYDx9NcunC462/b+Zz61NJHllVj0q+8Hfhoo8lec807w4l+eYxhmhhX40xDk9zzAkjS2EO7s2mrbjsaPqFeXVml17dXFV/UlXPHWNct+WlH0zy8iQ3T49vSnJZZr+od11xmar6B5I8brrm+/IkF2Z26c2jq+qHkvz1GONlmQXUm6vq85mFyuVV9VWZXU72rMzuhXlvVd2a5N+TvCPJ45M8oareOcb45XzxBPOa6d3yV4wxbpnuZ7g5sxPV68cY3kFfXS+qqh/L7PLFOzObN2+pqs/k/74zfVWS92R2Mnj3tO1IkldOc/GmbGO61OflSa6dLu15MLPLKj9yAMfCGhhjvLOqDlXVkczuoXv7cV43quqlmc2tY0luy2xuLe7nkmnFZSS5I8mun94IwPoplwnDeptC+GvGGK9a9lgAAE6WS8UAAID2rLgAAADtWXEBAADaEy4AAEB7LT5V7KnXvtH1atu4+GF3LXsIB+L13/Xqlv/i+lkXXmEebuOeo4eXPYQD8aVnxDxk6e6/7XC7eWgObpaOczAxDzfNXuehFRcAAKA94QIAALQnXAAAgPaECwAA0J5wAQAA2hMuAABAe8IFAABoT7gAAADtCRcAAKA94QIAALQnXAAAgPaECwAA0J5wAQAA2hMuAABAe8IFAABoT7gAAADtCRcAAKA94QIAALQnXAAAgPaECwAA0J5wAQAA2hMuAABAe8IFAABoT7gAAADtCRcAAKA94QIAALQnXAAAgPaECwAA0J5wAQAA2hMuAABAe8IFAABoT7g0cNl95y97CAAt3HP08LKHAEBTZyx7AJtitzjZ9vlp28fPueUghgR7cvbFVyRxQsn+2W0u7fT8fD4CsHmEywHar5WU8+686AvfixiWZfGEUcRwovZrzizuR8QAbBbhcgAO8tKvecQIGJbJKgx7dZBzZL5vAQOwGdzjso8uu+/803a/yuIqDCyLE0aO556jh09b2ApogM1gxWUfLOvmeqsvdGD1hUXLmgdWXwDWnxWXU9ThE8GsvtCBE0Y6xGuHMQBwMITLKegQLXPihQ7Ey+bqFAydxgLA/hEuJ6lTtMydd+dFAoalO/viKwTMhukYCqfzHhsATg/hchI6Rssi8UIH4mUzdI+D7uMDYO+ECwAA0J5wOUHdV1vmrLrQgVWX9bYqqxmrMk4AdiZcTsCqRMuceKED8bKeVi0GVm28APx/wmWPVi1a5sQLHYiX9bKqEbCq4wZgRrjswapGy5x4oQPxsh5W/eR/1ccPsMmECwAA0J5w2cWqr7bMWXWhA6suq21dVivW5TgANo1wAQAA2hMuO1iX1ZY5qy50YNVlNa3bKsW6HQ/AJhAuAABAe8IFAABoT7gcx7pdJjbncjE6cLnYalnXy6rW9bgA1pVwAQAA2hMuAABAe8IFAABoT7gAAADtCRcAAKA94QIAALR3xrIH0NG6fhTy3Hl3XpRrHn7HsofBhlv2RyLff5uPwt2Ldf/I4HuOHl76XARgb6y4bGPdT+rX/fiA/bPuJ/XrfnwA60S4AAAA7QkXAACgPeECAAC0J1wAAID2hAsAANCecAEAANoTLsexrh8ZvK7HBRycdf3I4HU9LoB1JVwAAID2hAsAANCecNnBul1WtW7HA5w+63ZZ1bodD8AmEC4AAEB7wmUX67JKsS7HASzPuqxSrMtxAGwa4QIAALQnXPZg1VcrVn38QB+rvlqx6uMH2GTCBQAAaO+MZQ9gVcxXLS677/wlj2TvrLQAB2G+anHP0cNLHsneWWkBWH1WXE7QqsTAqowTWF2rEgOrMk4AdiZcAACA9oTLSei+mtF9fMD66L6a0X18AOydcDlJHePgmoff0XJcwHrrGAdnX3xFy3EBcPKEyynoFAmdxgJsnk6R0GksAOwf4XKKOgRDhzEAdAiGDmMA4GD4OOR9sKyPShYsQDfL+qhkwQKw/qy47KPTeY+JaAE6O533mIgWgM1gxeUAHOQKjGABVslBrsAIFoDNIlwO0GJknErEiBVg1S1GxqlEjFgB2FzC5TTZKT4uu+98cQJsjJ3i456jh8UJANtyj0sDogVgRrQAcDzCBQAAaE+4AAAA7QkXAACgPeECAAC0J1wAAID2hAsAANCecAEAANoTLgAAQHvCBQAAaE+4AAAA7QkXAACgPeECAAC0J1wAAID2hAsAANCecAEAANoTLgAAQHvCBQAAaE+4AAAA7QkXAACgPeECAAC0J1wAAID2hAsAANCecAEAANoTLgAAQHvCBQAAaE+4AAAA7QkXAACgPeECAAC0J1wAAID2hAsAANCecAEAANoTLgAAQHvCBQAAaE+4AAAA7dUYY9ljAAAA2JEVFwAAoD3hAgAAtCdcAACA9oQLAADQnnABAADaEy4AAEB7wgUAAGhPuAAAAO0JFwAAoD3hAgAAtCdcAACA9oQLAADQnnABAADaEy4AAEB7wgUAAGhPuAAAAO0JFwAAoD3hAgAAtCdcAACA9oQLAADQnnABAADaEy4AAEB7wgUAAGjvfwEEoxqb56a3oQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /datasets/home/78/378/cs252cas/Pose_RCNN/logs/shapes20180519T0343/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.7942 - rpn_class_loss: 0.0322 - rpn_bbox_loss: 0.6198 - mrcnn_class_loss: 0.4223 - mrcnn_bbox_loss: 0.3830 - mrcnn_mask_loss: 0.3371"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py:2142: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 93s - loss: 1.7875 - rpn_class_loss: 0.0320 - rpn_bbox_loss: 0.6185 - mrcnn_class_loss: 0.4203 - mrcnn_bbox_loss: 0.3810 - mrcnn_mask_loss: 0.3357 - val_loss: 1.0794 - val_rpn_class_loss: 0.0172 - val_rpn_bbox_loss: 0.4404 - val_mrcnn_class_loss: 0.2289 - val_mrcnn_bbox_loss: 0.1909 - val_mrcnn_mask_loss: 0.2020\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 1. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /datasets/home/78/378/cs252cas/Pose_RCNN/logs/shapes20180519T0343/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.9199 - rpn_class_loss: 0.0168 - rpn_bbox_loss: 0.4177 - mrcnn_class_loss: 0.1939 - mrcnn_bbox_loss: 0.1368 - mrcnn_mask_loss: 0.1547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py:2142: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 69s - loss: 0.9186 - rpn_class_loss: 0.0168 - rpn_bbox_loss: 0.4170 - mrcnn_class_loss: 0.1934 - mrcnn_bbox_loss: 0.1369 - mrcnn_mask_loss: 0.1545 - val_loss: 0.8802 - val_rpn_class_loss: 0.0163 - val_rpn_bbox_loss: 0.4411 - val_mrcnn_class_loss: 0.1821 - val_mrcnn_bbox_loss: 0.1082 - val_mrcnn_mask_loss: 0.1325\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
